{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be prepping the data for a recommendation system. We will be making a content based recommendation system and for that we need to use NLP on the details section of the data. Towards the end of this notebook we also take a look at using NLP on the category section and using that to make content based recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Deep\n",
      "[nltk_data]     Shahane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Deep\n",
      "[nltk_data]     Shahane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# downloading resources and importing libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../internshala_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title                  0\n",
       "Company                0\n",
       "Location               0\n",
       "Duration               0\n",
       "Stipend                0\n",
       "Apply By               0\n",
       "Applicants             0\n",
       "Skills Required       68\n",
       "Perks                  0\n",
       "Number of Openings     0\n",
       "Link                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the data once\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'details'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17824\\3362746900.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# lets check this out once\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\applications\\python 3.7.8\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'details'"
     ]
    }
   ],
   "source": [
    "# lets check this out once \n",
    "df[df.details.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the entry as it is only one entry \n",
    "df.dropna(subset = ['details'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape once again\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use the following features for creating filters in the app:\n",
    "\n",
    "1. Skills/Category\n",
    "2. Locations\n",
    "3. Compensation\n",
    "\n",
    "In the final app, the user will be choosing from the options provided or inputting the data themselves for these categories. Haven't decided on that part yet. Based on the profile ID they have entered, we will be using a content based recommendation system to recommend the user similar profiles. This similarity will be based on the details column.\n",
    "\n",
    "We will be creating functions to implement the following steps in this notebook : \n",
    "1. Normalize : Remove extra signs and numbers form the sentences (everything is already lowercase)\n",
    "2. Tokenize\n",
    "3. Remove extra space from all tokens\n",
    "4. Stem each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    '''\n",
    "    tokenizes a bunch of sentences after normalizing them and returns stemmed tokens.\n",
    "    \n",
    "    INPUT:\n",
    "    sentences - a paragraph that need to be tokenized\n",
    "    \n",
    "    OUTPUT:\n",
    "    tokens - list of stemmed tokens\n",
    "    \n",
    "    '''\n",
    "    # normalizing, tokenizing, lemmatizing \n",
    "    sentences = re.sub('\\W',' ',sentences) \n",
    "    sentences = re.sub('[0-9]',' ',sentences)\n",
    "\n",
    "    tokens = word_tokenize(sentences)\n",
    "    tokens = [i.strip() for i in tokens]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(df):\n",
    "    '''\n",
    "    returns a similarity matrix, in the form of a dataframe, between different internships by using the \n",
    "    details section of df.\n",
    "    \n",
    "    INPUT:\n",
    "    df - dataframe with 'details' as one of the columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    sim - similarity matrix(dataframe) with internship id as column and row labels \n",
    "    \n",
    "    '''\n",
    "    details = df['details']\n",
    "    vect = CountVectorizer(tokenizer= tokenize, stop_words = 'english')\n",
    "    tfidf = TfidfTransformer()\n",
    "    \n",
    "    mat = tfidf.fit_transform(vect.fit_transform(details).toarray()).toarray()\n",
    "    sim = np.dot(mat, mat.T)\n",
    "    sim = pd.DataFrame(sim, columns=df.id, index = df.id)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = similarity_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.to_csv('../data_for_notebooks/recommendation_matrix.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out the recommendations in the make_recs notebook, I have come back to this notebook to make some changes. I think that we can maybe make better recommendations and thus we will try a few things out with this data(since it isn't big at all).\n",
    "\n",
    "The few ways that I can think of right now to change how recommendations are made:\n",
    "1. Do not use Tfidf and but stem tokens\n",
    "2. Lemmatize the words instead of stemming them\n",
    "3. Use lemmatization and do not use tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do not use Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix_wo_tfidf(df):\n",
    "    '''\n",
    "    returns a similarity matrix, in the form of a dataframe, between different internships by using the \n",
    "    details section\n",
    "    \n",
    "    INPUT:\n",
    "    df - dataframe with 'details' as one of the columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    sim - similarity matrix(dataframe) with internship id as column and row labels \n",
    "    \n",
    "    '''\n",
    "    details = df['details']\n",
    "    vect = CountVectorizer(tokenizer= tokenize, stop_words = 'english')\n",
    "    \n",
    "    mat = vect.fit_transform(details).toarray()\n",
    "    sim = np.dot(mat, mat.T)\n",
    "    sim = pd.DataFrame(sim, columns=df.id, index = df.id)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_1 = similarity_matrix_wo_tfidf(df)\n",
    "sim_1.to_csv('../data_for_notebooks/recommendation_matrix_wo_tfidf.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lemmatize the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lem(sentences):\n",
    "    '''\n",
    "    tokenizes a bunch of sentences after normalizing it and returns lemmatized tokens.\n",
    "    \n",
    "    INPUT:\n",
    "    sentences - a paragraph that needs to be tokenized\n",
    "    \n",
    "    OUTPUT:\n",
    "    tokens - list of lemmatized tokens\n",
    "    \n",
    "    '''\n",
    "    # normalizing, tokenizing, lemmatizing \n",
    "    sentences = re.sub('\\W',' ',sentences) \n",
    "    sentences = re.sub('[0-9]',' ',sentences)\n",
    "\n",
    "    tokens = word_tokenize(sentences)\n",
    "    tokens = [i.strip() for i in tokens]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix_w_lem(df):\n",
    "    '''\n",
    "    returns a similarity matrix, in the form of a dataframe, between different internships by using the \n",
    "    details section\n",
    "    \n",
    "    INPUT:\n",
    "    df - dataframe with 'details' as one of the columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    sim - similarity matrix(dataframe) with internship id as column and row labels \n",
    "    \n",
    "    '''\n",
    "    details = df['details']\n",
    "    vect = CountVectorizer(tokenizer= tokenize_lem, stop_words = 'english')\n",
    "    tfidf = TfidfTransformer()\n",
    "    \n",
    "    mat = tfidf.fit_transform(vect.fit_transform(details).toarray()).toarray()\n",
    "    sim = np.dot(mat, mat.T)\n",
    "    sim = pd.DataFrame(sim, columns=df.id, index = df.id)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_2 = similarity_matrix_w_lem(df)\n",
    "sim_2.to_csv('../data_for_notebooks/recommendation_matrix_w_lem.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use lemmatization and do not use tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix_w_lem_wo_tfidf(df):\n",
    "    '''\n",
    "    returns a similarity matrix, in the form of a dataframe, between different internships by using the \n",
    "    details section\n",
    "    \n",
    "    INPUT:\n",
    "    df - dataframe with 'details' as one of the columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    sim - similarity matrix(dataframe) with internship id as column and row labels \n",
    "    '''\n",
    "    details = df['details']\n",
    "    vect = CountVectorizer(tokenizer= tokenize_lem, stop_words = 'english')\n",
    "    tfidf = TfidfTransformer()\n",
    "    \n",
    "    mat = vect.fit_transform(details).toarray()\n",
    "    sim = np.dot(mat, mat.T)\n",
    "    sim = pd.DataFrame(sim, columns=df.id, index = df.id)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_3 = similarity_matrix_w_lem_wo_tfidf(df)\n",
    "sim_3.to_csv('../data_for_notebooks/recommendation_matrix_w_lem_wo_tfidf.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After forming these 3 with the details section, we try an alternate approach and try to use the 'category' column to form the similarity matrix. This should give us more relevant results as there are a high no. of common categories and similar internships have really similar category names. Thus, in the make_recs notebook, we will try using this too and compare it with the results of the above 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix_cat(df):\n",
    "    '''\n",
    "    returns a similarity matrix, in the form of a dataframe, between different internships by using the \n",
    "    cat section\n",
    "    \n",
    "    INPUT:\n",
    "    df - dataframe with 'category' as one of the columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    sim - similarity matrix(dataframe) with internship id as column and row labels \n",
    "    '''\n",
    "    cats = df['category']\n",
    "    vect = CountVectorizer(tokenizer= tokenize, stop_words = 'english')\n",
    "    tfidf = TfidfTransformer()\n",
    "    \n",
    "    mat = vect.fit_transform(cats).toarray()\n",
    "    sim = np.dot(mat, mat.T)\n",
    "    sim = pd.DataFrame(sim, columns=df.id, index = df.id)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The above has been formed in a similar way to sim_1, i.e, without tfidf and with stemming the tokens.This is because when we actually compare the recommendations given using different similarity matrices, we see that sim_1 gives us the best results. Thus later, what we do is that we combine both of sim_1 and sim_cat to get relevant and interesting recommendations. If sim_cat had been made using tdidf then the value of its elements would have been really small compared to sim_1's and it would have been a little difficult trying to make a similarity matrix which uses both of these to make recommendations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cat = similarity_matrix_cat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cat.to_csv('../data_for_notebooks/recommendation_df_cat.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
